ì•Œê² ìŠµë‹ˆë‹¤. ìŒì„± ê´€ë ¨ ê¸°ëŠ¥ë“¤ì„ ì›¹ API ê¸°ë°˜ìœ¼ë¡œ ì „í™˜í•˜ê³ , gTTS ë° LLM ì„¤ì •ì„ í™•ì¥í•˜ê² ìŠµë‹ˆë‹¤.

**í•µì‹¬ ë³€ê²½ ì‚¬í•­:**

1.  **`speech_recognition` ì œê±°, `streamlit-webrtc` ë° STT API ì‚¬ìš©:**
    *   `speech_recognition` ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ PyAudio ì˜ì¡´ì„±ì„ ì œê±°í•©ë‹ˆë‹¤.  ì´ë¡œì¨ ì„¤ì¹˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ë” ì•ˆì •ì ì¸ ì›¹ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    *   `streamlit-webrtc`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ì˜¤ë””ì˜¤ë¥¼ ë…¹ìŒí•©ë‹ˆë‹¤.  ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” Streamlitê³¼ ì˜ í†µí•©ë˜ë©° ë³„ë„ì˜ ì„œë²„ ì„¤ì • ì—†ì´ ì‘ë™í•©ë‹ˆë‹¤.
    *   ë…¹ìŒëœ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ Google Cloud Speech-to-Text API, AssemblyAI, Whisper API ë“± *ìƒìš© STT API*ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.  (ì˜ˆì œì—ì„œëŠ” Google Cloud STTë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë‹¤ë¥¸ APIë¡œ ì‰½ê²Œ êµì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)  ì›¹ APIë¥¼ ì‚¬ìš©í•˜ë©´ ë” ë†’ì€ ì •í™•ë„ì™€ ì•ˆì •ì„±ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        *  Google Cloud STTë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ Google Cloud Platform (GCP) í”„ë¡œì íŠ¸ë¥¼ ìƒì„±í•˜ê³ , Speech-to-Text APIë¥¼ í™œì„±í™”í•˜ê³ , ì„œë¹„ìŠ¤ ê³„ì • í‚¤ (JSON íŒŒì¼)ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.  ì´ í‚¤ íŒŒì¼ì„ í”„ë¡œì íŠ¸ì— ì¶”ê°€í•˜ê³  í™˜ê²½ ë³€ìˆ˜ `GOOGLE_APPLICATION_CREDENTIALS`ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.

2.  **gTTS ì„¤ì • í™•ì¥:**
    *   ì„±ë³„ (ë‚¨ì„±/ì—¬ì„±) ë° ì†ë„ (ëŠë¦¼/ë³´í†µ/ë¹ ë¦„)ë¥¼ ì„ íƒí•  ìˆ˜ ìˆëŠ” UI ìš”ì†Œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    *   gTTS ê°ì²´ë¥¼ ìƒì„±í•  ë•Œ ì´ëŸ¬í•œ ì„¤ì •ì„ ë°˜ì˜í•©ë‹ˆë‹¤.

3.  **LLM ì„¤ì • í™•ì¥:**
    *   `top_p`, `top_k`, `repeat_penalty`ì™€ ê°™ì€ ì¶”ê°€ì ì¸ LLM íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆëŠ” UI ìš”ì†Œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    *   `call_ollama_api` í•¨ìˆ˜ì—ì„œ ì´ëŸ¬í•œ íŒŒë¼ë¯¸í„°ë¥¼ `options`ì— í¬í•¨ì‹œì¼œ Ollama APIì— ì „ë‹¬í•©ë‹ˆë‹¤.

4.  **ì½”ë“œ ì •ë¦¬ ë° ëª¨ë“ˆí™”:**
    *   STT, TTS ê´€ë ¨ í•¨ìˆ˜ë¥¼ ë³„ë„ì˜ ëª¨ë“ˆ (`stt_tts.py`)ë¡œ ë¶„ë¦¬í•˜ì—¬ ì½”ë“œë¥¼ ë” ê¹”ë”í•˜ê²Œ êµ¬ì„±í•©ë‹ˆë‹¤.
    *   `utils.py` ì— ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë  í•¨ìˆ˜ë¥¼ ë„£ì–´ë‘ .

**ì „ì²´ ì½”ë“œ (êµ¬ì¡° ë³€ê²½):**

ë‹¤ìŒì€ í”„ë¡œì íŠ¸ì˜ êµ¬ì¡°ì…ë‹ˆë‹¤.

```
should_i/
â”œâ”€â”€ app.py           # ë©”ì¸ Streamlit ì•±
â”œâ”€â”€ stt_tts.py        # STT, TTS ê´€ë ¨ í•¨ìˆ˜
â”œâ”€â”€ utils.py        # ê³µí†µ í•¨ìˆ˜
â””â”€â”€ requirements.txt  # í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª©ë¡
```

**requirements.txt:**

```
streamlit
requests
gTTS
duckduckgo-search
streamlit-webrtc
google-cloud-speech # Google Cloud STT ì‚¬ìš© ì‹œ
# assemblyai  # AssemblyAI ì‚¬ìš© ì‹œ
# openai      # Whisper API ì‚¬ìš© ì‹œ
```

**utils.py:**
```python
import base64

def encode_image(image_bytes):
    """Encodes image bytes to base64."""
    return base64.b64encode(image_bytes).decode("utf-8")
```

**stt_tts.py:**

```python
import io
import streamlit as st
from gtts import gTTS
from google.cloud import speech  # Google Cloud STT


def google_cloud_stt(audio_bytes, language_code):
    """Transcribes audio bytes using Google Cloud Speech-to-Text."""
    try:
        client = speech.SpeechClient()
        audio = speech.RecognitionAudio(content=audio_bytes)
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            # sample_rate_hertz=44100,  # Adjust if needed
            sample_rate_hertz=48000,
            language_code=language_code,
            enable_automatic_punctuation=True, # êµ¬ë‘ì  ì¶”ê°€.
        )

        response = client.recognize(config=config, audio=audio)
        if response.results:
            return response.results[0].alternatives[0].transcript
        else:
            return None

    except Exception as e:
        st.error(f"Google Cloud STT Error: {e}")
        return None

# --- ë‹¤ë¥¸ STT API ì‚¬ìš© ì˜ˆì‹œ (AssemblyAI, Whisper) ---
# def assemblyai_stt(audio_bytes, api_key):
#    # AssemblyAI API implementation (requires assemblyai package)
#     ...

# def whisper_stt(audio_bytes, api_key):
#     # OpenAI Whisper API implementation (requires openai package)
#     ...


def text_to_speech(text, language="ko", gender="female", speed="normal"):
    """Converts text to speech using gTTS with gender and speed options."""
    try:
        # gTTSì—ì„œ ì†ë„ ì¡°ì ˆì€ slow=True/Falseë¡œë§Œ ê°€ëŠ¥.  ë” ì„¸ë°€í•œ ì¡°ì ˆì€ ë‹¤ë¥¸ TTS ì—”ì§„ í•„ìš”.
        slow = speed == "slow"
        tts = gTTS(text=text, lang=language, slow=slow, tld="com") # tld ì„¤ì •

        # ì„±ë³„ ì„¤ì •ì€ gTTSì—ì„œ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ.  ë‹¤ë¥¸ TTS ì—”ì§„ (ì˜ˆ: Google Cloud Text-to-Speech) í•„ìš”.
        # ì—¬ê¸°ì„œëŠ” gTTSë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, ì„±ë³„ì€ ë¬´ì‹œë¨.  ë” ë‚˜ì€ ì„±ë³„ ì œì–´ë¥¼ ì›í•˜ë©´ ë‹¤ë¥¸ TTS API ì‚¬ìš©.

        buffer = io.BytesIO()
        tts.write_to_fp(buffer)
        buffer.seek(0)
        return buffer.read()
    except Exception as e:
        st.error(f"gTTS ì˜¤ë¥˜: {e}")
        return None

```

**app.py:**

```python
import streamlit as st
import requests
from duckduckgo_search import DDGS
from stt_tts import google_cloud_stt, text_to_speech  # Import STT and TTS functions
from utils import encode_image
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import av

# --- Constants ---
OLLAMA_HOST = "http://192.168.0.119:11434"  # Replace with your Ollama server
OLLAMA_MODEL = "llama3.2-vision"
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ì™€ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì„¸ìš”.
ë‹¤ìŒ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µì„ ì œê³µí•˜ì„¸ìš”:

- "probability": "ì˜ˆ" (í•´ì•¼ í•¨/êµ¬ë§¤í•´ì•¼ í•¨) ëŒ€ "ì•„ë‹ˆì˜¤" (í•˜ì§€ ë§ì•„ì•¼ í•¨/êµ¬ë§¤í•˜ì§€ ë§ì•„ì•¼ í•¨)ì˜ ê°€ëŠ¥ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ë°±ë¶„ìœ¨ (0-100)ì…ë‹ˆë‹¤.
                   ë” ë†’ì€ ë°±ë¶„ìœ¨ì€ "ì˜ˆ"ë¥¼ ì˜ë¯¸í•˜ê³ , ë” ë‚®ì€ ë°±ë¶„ìœ¨ì€ "ì•„ë‹ˆì˜¤"ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
- "reason": í•´ë‹¹ë˜ëŠ” ê²½ìš° ì¶œì²˜ë¥¼ ì¸ìš©í•˜ì—¬ ë¶„ì„ ë° ì¶”ë¡ ì— ëŒ€í•œ ê°„ê²°í•œ ì„¤ëª….

ì˜ˆì‹œ:
{"probability": 75, "reason": "í˜„ì¬ ì‹œì¥ ë™í–¥ê³¼ ì „ë¬¸ê°€ ì˜ê²¬ì— ë”°ë¥´ë©´, í•´ë‹¹ ì£¼ì‹ì€ ê°•ë ¥í•œ ì„±ì¥ ì ì¬ë ¥ì„ ë³´ì…ë‹ˆë‹¤."}
"""

# --- Helper Functions ---

def perform_ddg_search(query, max_results=3):
    """Performs DuckDuckGo search; handles empty queries."""
    if not query:
        return ""
    try:
        with DDGS() as ddgs:
            results = [r["body"] for r in ddgs.text(query, max_results=max_results)]
        return "\n\n".join(results)
    except Exception as e:
        st.error(f"DuckDuckGo ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return ""


def call_ollama_api(messages, stream=False, format="json"):
    """Calls the Ollama API."""
    data = {
        "model": OLLAMA_MODEL,
        "messages": messages,
        "stream": stream,
        "format": format,
        "options": {
            "temperature": st.session_state.temperature,
            "num_predict": st.session_state.max_tokens,
            "top_p": st.session_state.top_p,
            "top_k": st.session_state.top_k,
            "repeat_penalty": st.session_state.repeat_penalty,
        },
    }
    try:
        response = requests.post(f"{OLLAMA_HOST}/api/chat", json=data)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"Ollama API ì˜¤ë¥˜: {e}")
        return None



# --- Analysis Functions ---

def analyze_image(image_data, question, language):
    """Analyzes image and question using Ollama."""
    if question:
        search_results = perform_ddg_search(question, max_results=2)
        combined_input = f"{question}\n\nê´€ë ¨ ì •ë³´:\n{search_results}"
    else:
        combined_input = "ì‚¬ì§„ì— ëŒ€í•´ ë¶„ì„í•´ì£¼ì„¸ìš”."

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": combined_input, "images": [image_data]},
    ]
    response_json = call_ollama_api(messages)
    return process_ollama_response(response_json, language) if response_json else (None, "ì˜¤ë¥˜: Ollama API í˜¸ì¶œ ì‹¤íŒ¨.", None)


def analyze_text(question, language):
    """Analyzes text question using Ollama."""
    search_results = perform_ddg_search(question)
    combined_input = f"{question}\n\nê´€ë ¨ ì •ë³´:\n{search_results}"
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": combined_input},
    ]
    response_json = call_ollama_api(messages)
    return process_ollama_response(response_json, language) if response_json else (None, "ì˜¤ë¥˜: Ollama API í˜¸ì¶œ ì‹¤íŒ¨.", None)

def process_ollama_response(response_json, language):
    """Processes the Ollama API response."""
    try:
        content_str = response_json['message']['content']
        content_json = json.loads(content_str)
        probability = content_json.get("probability", None)
        reason = content_json.get("reason", None)

        if probability is not None and not (0 <= probability <= 100):
            raise ValueError("í™•ë¥ ì´ 0-100 ë²”ìœ„ ë‚´ì— ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

        audio_bytes = text_to_speech(
            reason, language, st.session_state.tts_gender, st.session_state.tts_speed
        ) if reason else None

        return probability, reason, audio_bytes
    except (json.JSONDecodeError, KeyError, ValueError) as e:
        st.error(f"Ollama ì‘ë‹µ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None, "ì˜¤ë¥˜: Ollamaë¡œë¶€í„° ìœ íš¨í•˜ì§€ ì•Šì€ ì‘ë‹µ.", None


# --- Streamlit App ---

st.set_page_config(page_title="í•´ì•¼ í• ê¹Œìš”...?", page_icon="ğŸ¤”", layout="wide")

# --- Custom CSS ---
st.markdown("""
<style>
/* Main container */
.main {
    padding: 2rem;
    font-family: sans-serif;
}

/* Title */
.title {
    font-size: 3rem;
    font-weight: bold;
    margin-bottom: 1rem;
    color: #333;
}

/* Subtitle/Description */
.subtitle {
    font-size: 1.25rem;
    color: #555;
    margin-bottom: 2rem;
}

/* Input area */
.stTextInput, .stFileUploader, .stCameraInput, .stRadio {
    margin-bottom: 1rem;
}

/* Buttons */
.stButton>button {
    background-color: #007bff;
    color: white;
    border: none;
    padding: 0.75rem 1.5rem;
    border-radius: 0.5rem;
    font-weight: bold;
    transition: background-color 0.2s ease;
}
.stButton>button:hover {
    background-color: #0056b3;
}
.stButton>button:focus {
    outline: none;
    box-shadow: 0 0 0 3px rgba(0, 123, 255, 0.25);
}

/* Radio buttons */
.stRadio > div > label {
  margin-right: 1rem;
  padding: 0.5rem 1rem;
  border-radius: 0.5rem;
  border: 1px solid #ccc;
  transition: all 0.2s ease;
}
.stRadio > div > label[data-baseweb="radio"] > div:first-child {
    background-color: #007bff !important;
    border-color: #007bff !important;
}
.stRadio > div > label[data-baseweb="radio"] {
    background-color: #f0f2f6;
    border-color: #f0f2f6;
}

/* Result area */
.result-area {
    padding: 1.5rem;
    border-radius: 0.75rem;
    background-color: #f8f9fa;
    border: 1px solid #e9ecef;
}

/* Success/Error messages */
.st-success, .st-error {
    padding: 1rem;
    border-radius: 0.5rem;
    margin-bottom: 1rem;
    font-weight: bold;
    color: white;
    display: flex;
    align-items: center;
}
.st-success {
    background-color: #28a745;
}
.st-error {
    background-color: #dc3545;
}
.st-success svg, .st-error svg {
    margin-right: 0.5rem;
    font-size: 1.2em;
}

/* Expander */
.st-expander {
  border: none !important;
}
.st-expanderHeader {
    font-weight: bold;
    background-color: transparent;
    padding: 0.5rem 0;
}
.st-expanderContent {
    padding: 0.5rem 0;
}

/* Gauge/Progress */
.stProgress > div > div > div > div {
    background-color: #007bff;
}

/* Sidebar */
.sidebar .sidebar-content {
    padding: 1rem;
    background-color: #f0f2f6;
}
</style>
""", unsafe_allow_html=True)

# --- Session State ---
if "language" not in st.session_state:
    st.session_state["language"] = "ko"
if "max_tokens" not in st.session_state:
    st.session_state["max_tokens"] = 256
if "temperature" not in st.session_state:
    st.session_state["temperature"] = 0.7
if "top_p" not in st.session_state:
    st.session_state["top_p"] = 0.9
if "top_k" not in st.session_state:
    st.session_state["top_k"] = 50
if "repeat_penalty" not in st.session_state:
    st.session_state["repeat_penalty"] = 1.1
if "tts_gender" not in st.session_state:  # TTS ì„¤ì •
    st.session_state["tts_gender"] = "female"
if "tts_speed" not in st.session_state:
    st.session_state["tts_speed"] = "normal"



# --- Sidebar ---
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    st.session_state.language = st.selectbox("ì–¸ì–´", ["ko", "en", "ja", "zh-CN", "fr", "de"], index=0)
    language_code = st.session_state.language.split("-")[0]

    with st.expander("LLM ì„¤ì •"):
        st.session_state.max_tokens = st.slider("ìµœëŒ€ í† í° ìˆ˜", 1, 2048, 256, 1)
        st.session_state.temperature = st.slider("ì˜¨ë„", 0.1, 4.0, 0.7, 0.1)
        st.session_state.top_p = st.slider("Top P", 0.1, 1.0, 0.9, 0.05)
        st.session_state.top_k = st.slider("Top K", 1, 100, 50, 1)
        st.session_state.repeat_penalty = st.slider("ë°˜ë³µ í˜ë„í‹°", 0.1, 2.0, 1.1, 0.05)

    with st.expander("TTS ì„¤ì •"):
        st.session_state.tts_gender = st.selectbox("ëª©ì†Œë¦¬ ì„±ë³„", ["female", "male"], index=0)  # gTTSëŠ” ì œí•œì 
        st.session_state.tts_speed = st.select_slider("ì½ê¸° ì†ë„", ["slow", "normal", "fast"], value="normal")


# --- Main App ---
st.markdown("<h1 class='title'>í•´ì•¼ í• ê¹Œìš”...? ğŸ¤”</h1>", unsafe_allow_html=True)
st.markdown("<p class='subtitle'>ì§ˆë¬¸ì„ í•˜ê³  í™•ë¥  ê¸°ë°˜ ë¶„ì„ì„ ë°›ìœ¼ì„¸ìš”!</p>", unsafe_allow_html=True)

col1, col2 = st.columns([3, 1])

with col1:
    input_type = st.radio("ì…ë ¥ ìœ í˜•", ["í…ìŠ¤íŠ¸", "ìŒì„±", "ì´ë¯¸ì§€ ì—…ë¡œë“œ", "ì‚¬ì§„ ì´¬ì˜"], horizontal=True)
    question = ""
    audio_bytes = None # ìŒì„± ë°”ì´íŠ¸

    if input_type == "í…ìŠ¤íŠ¸":
        question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ì´ ì£¼ì‹ì„ ì‚¬ì•¼ í• ê¹Œìš”?")

    elif input_type == "ìŒì„±":
        # webrtc_streamerë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ë…¹ìŒ
        webrtc_ctx = webrtc_streamer(
            key="speech-recording",
            mode=WebRtcMode.SENDONLY,
            audio_receiver_size=1024,
            # rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}, # í•„ìš” ì‹œ STUN ì„œë²„ ì„¤ì •
            media_stream_constraints={"video": False, "audio": True},
        )

        if webrtc_ctx.state.playing:
            st.write("ğŸ¤ ë…¹ìŒ ì¤‘...")

        if webrtc_ctx.audio_receiver:
            try:
                audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=1)
                if audio_frames:
                    # ìµœì‹  í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
                    latest_frame = audio_frames[-1]
                    audio_bytes = latest_frame.to_ndarray().tobytes()


                    if audio_bytes:
                        question = google_cloud_stt(audio_bytes, language_code) # STT
                        if question:
                            st.write("ë‹¹ì‹ ì˜ ì§ˆë¬¸:", question)
            except Exception as e:
                 st.error(f"ì˜¤ë¥˜: {e}")

    if input_type == "ì´ë¯¸ì§€ ì—…ë¡œë“œ":
        question = st.text_input("ì§ˆë¬¸ (ì„ íƒ ì‚¬í•­):", placeholder="ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒ ì‚¬í•­).")
        uploaded_image = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"], label_visibility='collapsed')
        camera_image = None
    elif input_type == "ì‚¬ì§„ ì´¬ì˜":
        question = st.text_input("ì§ˆë¬¸ (ì„ íƒ ì‚¬í•­):", placeholder="ì‚¬ì§„ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒ ì‚¬í•­).")
        camera_image = st.camera_input("ì‚¬ì§„ ì´¬ì˜", label_visibility="collapsed")
        uploaded_image = None
    else:
        uploaded_image = None
        camera_image = None


    if st.button("ë¶„ì„", type="primary", use_container_width=True):
        if input_type == "ì´ë¯¸ì§€ ì—…ë¡œë“œ" and not uploaded_image:
             st.warning("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        elif input_type == "ì‚¬ì§„ ì´¬ì˜" and not camera_image:
            st.warning("ì‚¬ì§„ì„ ì´¬ì˜í•´ì£¼ì„¸ìš”")
        elif not question and input_type in ("í…ìŠ¤íŠ¸", "ìŒì„±") :
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜, ìŒì„±ì„ ë…¹ìŒí•´ì£¼ì„¸ìš”.")

        else:
            with st.spinner("ë¶„ì„ ì¤‘..."):
                if input_type in ("í…ìŠ¤íŠ¸", "ìŒì„±") and question:
                    probability, reason, audio = analyze_text(question, st.session_state.language)
                elif input_type == "ì´ë¯¸ì§€ ì—…ë¡œë“œ" and uploaded_image:
                    image_bytes = uploaded_image.getvalue()
                    image_base64 = encode_image(image_bytes)
                    probability, reason, audio = analyze_image(image_base64, question, st.session_state.language)
                elif input_type == "ì‚¬ì§„ ì´¬ì˜" and camera_image:
                    image_bytes = camera_image.getvalue()
                    image_base64 = encode_image(image_bytes)
                    probability, reason, audio = analyze_image(image_base64, question, st.session_state.language)

                else:
                    probability, reason, audio = None, "ì…ë ¥ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None

                with col2:
                    if probability is not None and reason:
                        st.subheader("ë¶„ì„ ê²°ê³¼")
                        with st.container(border=True):
                            if probability >= 50:
                                st.success(f"âœ… ì˜ˆ! ({probability}%)")
                            else:
                                st.error(f"âŒ ì•„ë‹ˆì˜¤! ({probability}%)")
                            st.progress(probability / 100.0)
                            with st.expander("ì´ìœ ", expanded=True):
                                st.markdown(reason)
                            if audio:
                                st.audio(audio, format="audio/mp3")
                    elif reason:
                        st.error(reason)
```

**ì‹¤í–‰ ë°©ë²• ë° ì£¼ì˜ ì‚¬í•­:**

1.  **í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜:** `pip install -r requirements.txt`
2.  **Google Cloud Speech-to-Text ì„¤ì • (Google Cloud STT ì‚¬ìš© ì‹œ):**
    *   GCP í”„ë¡œì íŠ¸ ìƒì„± ë° Speech-to-Text API í™œì„±í™”.
    *   ì„œë¹„ìŠ¤ ê³„ì • í‚¤ (JSON íŒŒì¼) ìƒì„±.
    *   `GOOGLE_APPLICATION_CREDENTIALS` í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ê²½ë¡œë¡œ ì„¤ì •.  (ë˜ëŠ” ì½”ë“œ ë‚´ì—ì„œ `os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "path/to/your/key.json"`ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ, ë³´ì•ˆìƒ ê¶Œì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)
3.  **Ollama ì„œë²„ ì‹¤í–‰:** Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ê³  `OLLAMA_HOST`ì— ì˜¬ë°”ë¥¸ ì£¼ì†Œê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
4.  **Streamlit ì•± ì‹¤í–‰:** `streamlit run app.py`
5.  **HTTPS (ê¶Œì¥):** `streamlit-webrtc`ëŠ” ë¡œì»¬ í™˜ê²½ (`localhost`)ì—ì„œëŠ” HTTPë¡œ ì‘ë™í•˜ì§€ë§Œ, ë°°í¬ ì‹œì—ëŠ” HTTPSê°€ í•„ìš”í•©ë‹ˆë‹¤.  HTTPSë¥¼ ì‚¬ìš©í•˜ë©´ ë¸Œë¼ìš°ì €ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ ë” ì‰½ê²Œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ì œ ì•±ì€ ì›¹ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„± ì…ë ¥ì„ ì²˜ë¦¬í•˜ê³ , gTTS ë° LLM ì„¤ì •ì´ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.  Google Cloud STTë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  ë‹¤ë¥¸ STT API (AssemblyAI, Whisper API ë“±)ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `stt_tts.py` íŒŒì¼ì—ì„œ í•´ë‹¹ APIë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤.
