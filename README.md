![image](https://github.com/user-attachments/assets/9e6b567e-a9e8-49a2-bc29-0f1c2bf73f3f)
ì•Œê² ìŠµë‹ˆë‹¤. ìµœëŒ€í•œ ì›¹ ë¸Œë¼ìš°ì € APIë¥¼ í™œìš©í•˜ê³ , ì™¸ë¶€ API ì˜ì¡´ì„±ì„ ìµœì†Œí™”í•˜ì—¬ `app.py` íŒŒì¼ í•˜ë‚˜ë¡œ ë™ì‘í•˜ëŠ” ë²„ì „ì„ ë§Œë“¤ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ìˆ˜ì •ëœ ì½”ë“œì…ë‹ˆë‹¤.

```python
import streamlit as st
import requests
import base64
import json
import io
from gtts import gTTS  # gTTSëŠ” ì—¬ì „íˆ ì‚¬ìš© (ì›¹ API ëŒ€ì•ˆì´ ì œí•œì )
from duckduckgo_search import DDGS
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import av
import logging

# ë¡œê¹… ì„¤ì • (webrtc ê´€ë ¨ ì˜¤ë¥˜ë¥¼ ë³´ê¸° ìœ„í•¨)
logging.basicConfig(level=logging.DEBUG)

# --- ìƒìˆ˜ ---
OLLAMA_HOST = "http://192.168.0.119:11434"  # Ollama ì„œë²„ ì£¼ì†Œ
OLLAMA_MODEL = "llama3.2-vision"
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ì™€ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì„¸ìš”.
ë‹¤ìŒ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µì„ ì œê³µí•˜ì„¸ìš”:

- "probability": "ì˜ˆ" (í•´ì•¼ í•¨/êµ¬ë§¤í•´ì•¼ í•¨) ëŒ€ "ì•„ë‹ˆì˜¤" (í•˜ì§€ ë§ì•„ì•¼ í•¨/êµ¬ë§¤í•˜ì§€ ë§ì•„ì•¼ í•¨)ì˜ ê°€ëŠ¥ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ë°±ë¶„ìœ¨ (0-100)ì…ë‹ˆë‹¤.
                   ë” ë†’ì€ ë°±ë¶„ìœ¨ì€ "ì˜ˆ"ë¥¼ ì˜ë¯¸í•˜ê³ , ë” ë‚®ì€ ë°±ë¶„ìœ¨ì€ "ì•„ë‹ˆì˜¤"ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
- "reason": í•´ë‹¹ë˜ëŠ” ê²½ìš° ì¶œì²˜ë¥¼ ì¸ìš©í•˜ì—¬ ë¶„ì„ ë° ì¶”ë¡ ì— ëŒ€í•œ ê°„ê²°í•œ ì„¤ëª….

ì˜ˆì‹œ:
{"probability": 75, "reason": "í˜„ì¬ ì‹œì¥ ë™í–¥ê³¼ ì „ë¬¸ê°€ ì˜ê²¬ì— ë”°ë¥´ë©´, í•´ë‹¹ ì£¼ì‹ì€ ê°•ë ¥í•œ ì„±ì¥ ì ì¬ë ¥ì„ ë³´ì…ë‹ˆë‹¤."}
"""

# --- ë„ìš°ë¯¸ í•¨ìˆ˜ ---

def text_to_speech(text, language="ko", gender="female", speed="normal"):
    """gTTSë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤ (ì„±ë³„, ì†ë„ ì˜µì…˜ í¬í•¨)."""
    try:
        slow = speed == "slow"
        tts = gTTS(text=text, lang=language, slow=slow, tld="com") # tld ì„¤ì •.

        # ì„±ë³„ ì„¤ì •ì€ gTTSì—ì„œ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ. ë‹¤ë¥¸ TTSì—”ì§„ í•„ìš”.

        buffer = io.BytesIO()
        tts.write_to_fp(buffer)
        buffer.seek(0)
        return buffer.read()
    except Exception as e:
        st.error(f"gTTS ì˜¤ë¥˜: {e}")
        return None

def encode_image(image_bytes):
    """ì´ë¯¸ì§€ ë°”ì´íŠ¸ë¥¼ base64ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤."""
    return base64.b64encode(image_bytes).decode("utf-8")

def perform_ddg_search(query, max_results=3):
    """DuckDuckGo ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ì—°ê²°ëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (ë¹ˆ ì¿¼ë¦¬ ì²˜ë¦¬)."""
    if not query:
        return ""
    try:
        with DDGS() as ddgs:
            results = [r["body"] for r in ddgs.text(query, max_results=max_results)]
        return "\n\n".join(results)
    except Exception as e:
        st.error(f"DuckDuckGo ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return ""

def call_ollama_api(messages, stream=False, format="json"):
    """Ollama APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤ (í™•ì¥ëœ LLM ì˜µì…˜ í¬í•¨)."""
    data = {
        "model": OLLAMA_MODEL,
        "messages": messages,
        "stream": stream,
        "format": format,
        "options": {
            "temperature": st.session_state.temperature,
            "num_predict": st.session_state.max_tokens,
            "top_p": st.session_state.top_p,
            "top_k": st.session_state.top_k,
            "repeat_penalty": st.session_state.repeat_penalty,
        },
    }
    try:
        response = requests.post(f"{OLLAMA_HOST}/api/chat", json=data)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"Ollama API ì˜¤ë¥˜: {e}")
        return None

# --- ë¶„ì„ í•¨ìˆ˜ ---
def analyze_image(image_data, question, language):
    """ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì„ ë¶„ì„í•©ë‹ˆë‹¤ (Ollama ì‚¬ìš©, ì„ íƒì  DuckDuckGo ê²€ìƒ‰)."""
    if question:
        search_results = perform_ddg_search(question, max_results=2)
        combined_input = f"{question}\n\nê´€ë ¨ ì •ë³´:\n{search_results}"
    else:
        combined_input = "ì‚¬ì§„ì— ëŒ€í•´ ë¶„ì„í•´ì£¼ì„¸ìš”."

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": combined_input, "images": [image_data]},
    ]
    response_json = call_ollama_api(messages)
    return process_ollama_response(response_json, language) if response_json else (None, "ì˜¤ë¥˜: Ollama API í˜¸ì¶œ ì‹¤íŒ¨.", None)

def analyze_text(question, language):
    """í…ìŠ¤íŠ¸ ì§ˆë¬¸ì„ ë¶„ì„í•©ë‹ˆë‹¤ (Ollama ì‚¬ìš©, DuckDuckGo ê²€ìƒ‰)."""
    search_results = perform_ddg_search(question)
    combined_input = f"{question}\n\nê´€ë ¨ ì •ë³´:\n{search_results}"
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": combined_input},
    ]
    response_json = call_ollama_api(messages)
    return process_ollama_response(response_json, language) if response_json else (None, "ì˜¤ë¥˜: Ollama API í˜¸ì¶œ ì‹¤íŒ¨.", None)

def process_ollama_response(response_json, language):
    """Ollama API ì‘ë‹µì„ ì²˜ë¦¬í•˜ê³ , ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³ , TTSë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        content_str = response_json['message']['content']
        content_json = json.loads(content_str)
        probability = content_json.get("probability", None)
        reason = content_json.get("reason", None)

        if probability is not None and not (0 <= probability <= 100):
            raise ValueError("í™•ë¥ ì´ 0-100 ë²”ìœ„ ë‚´ì— ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # ì„ íƒëœ ì„±ë³„ê³¼ ì†ë„ë¡œ TTS ìƒì„±
        audio_bytes = text_to_speech(
            reason, language, st.session_state.tts_gender, st.session_state.tts_speed
        ) if reason else None

        return probability, reason, audio_bytes
    except (json.JSONDecodeError, KeyError, ValueError) as e:
        st.error(f"Ollama ì‘ë‹µ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None, "ì˜¤ë¥˜: Ollamaë¡œë¶€í„° ìœ íš¨í•˜ì§€ ì•Šì€ ì‘ë‹µ.", None

# --- Streamlit ì•± ---

st.set_page_config(page_title="í•´ì•¼ í• ê¹Œìš”...?", page_icon="ğŸ¤”", layout="wide")

# --- Custom CSS ---
st.markdown("""
<style>
/* (ì´ì „ CSS ìŠ¤íƒ€ì¼) */
/* Main container */
.main {
    padding: 2rem;
    font-family: sans-serif; /* Modern font */
}

/* Title */
.title {
    font-size: 3rem;
    font-weight: bold;
    margin-bottom: 1rem;
    color: #333;  /* Darker title color */
}

/* Subtitle/Description */
.subtitle {
    font-size: 1.25rem;
    color: #555;
    margin-bottom: 2rem;
}

/* Input area */
.stTextInput, .stFileUploader, .stCameraInput, .stRadio {
    margin-bottom: 1rem;
}

/* Buttons */
.stButton>button {
    background-color: #007bff; /* Primary blue color */
    color: white;
    border: none;
    padding: 0.75rem 1.5rem;
    border-radius: 0.5rem; /* Rounded corners */
    font-weight: bold;
    transition: background-color 0.2s ease; /* Smooth hover effect */
}
.stButton>button:hover {
    background-color: #0056b3; /* Darker blue on hover */
}
.stButton>button:focus {
    outline: none;  /* Remove the focus outline */
    box-shadow: 0 0 0 3px rgba(0, 123, 255, 0.25); /* Add a subtle focus ring */
}

/* Radio buttons */
.stRadio > div > label { /* Target radio button labels */
  margin-right: 1rem; /* Add spacing */
  padding: 0.5rem 1rem;
  border-radius: 0.5rem; /* Rounded */
  border: 1px solid #ccc;
  transition: all 0.2s ease;  /* Smooth transition */
}
/* Style the selected radio option */
.stRadio > div > label[data-baseweb="radio"] > div:first-child {
    background-color: #007bff !important;  /* Override to ensure color */
    border-color: #007bff !important;
}
.stRadio > div > label[data-baseweb="radio"] {
    background-color: #f0f2f6;
    border-color: #f0f2f6;
}


/* Result area */
.result-area {
    padding: 1.5rem;
    border-radius: 0.75rem;
    background-color: #f8f9fa; /* Light background */
    border: 1px solid #e9ecef;  /* Subtle border */
}

/* Success/Error messages */
.st-success, .st-error {
    padding: 1rem;
    border-radius: 0.5rem;
    margin-bottom: 1rem;
    font-weight: bold;
    color: white;
    display: flex;
    align-items: center;
}
.st-success {
    background-color: #28a745; /* Green */
}
.st-error {
    background-color: #dc3545; /* Red */
}
.st-success svg, .st-error svg { /* Icons within success/error */
    margin-right: 0.5rem;  /* Space the icon */
    font-size: 1.2em;
}


/* Expander */
.st-expander {
  border: none !important; /* No borders */
}
.st-expanderHeader {
    font-weight: bold;
    background-color: transparent;
    padding: 0.5rem 0;
}
.st-expanderContent {
    padding: 0.5rem 0;
}

/* Gauge/Progress */
.stProgress > div > div > div > div {
    background-color: #007bff; /* Consistent blue */
}

/* Sidebar */
.sidebar .sidebar-content {
    padding: 1rem;
    background-color: #f0f2f6; /* Lighter background */
}
</style>
""", unsafe_allow_html=True)

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "language" not in st.session_state:
    st.session_state["language"] = "ko"
if "max_tokens" not in st.session_state:
    st.session_state["max_tokens"] = 256
if "temperature" not in st.session_state:
    st.session_state["temperature"] = 0.7
if "top_p" not in st.session_state:
    st.session_state["top_p"] = 0.9
if "top_k" not in st.session_state:
    st.session_state["top_k"] = 50
if "repeat_penalty" not in st.session_state:
    st.session_state["repeat_penalty"] = 1.1
if "tts_gender" not in st.session_state:  # TTS ì„¤ì •
    st.session_state["tts_gender"] = "female"
if "tts_speed" not in st.session_state:
    st.session_state["tts_speed"] = "normal"
if "recorded_audio" not in st.session_state:  # ë…¹ìŒëœ ì˜¤ë””ì˜¤ ì €ì¥
    st.session_state["recorded_audio"] = None

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    st.session_state.language = st.selectbox("ì–¸ì–´", ["ko", "en", "ja", "zh-CN", "fr", "de"], index=0)
    language_code = st.session_state.language.split("-")[0]

    with st.expander("LLM ì„¤ì •"):
        st.session_state.max_tokens = st.slider("ìµœëŒ€ í† í° ìˆ˜", 1, 2048, 256, 1)
        st.session_state.temperature = st.slider("ì˜¨ë„", 0.1, 4.0, 0.7, 0.1)
        st.session_state.top_p = st.slider("Top P", 0.1, 1.0, 0.9, 0.05)
        st.session_state.top_k = st.slider("Top K", 1, 100, 50, 1)
        st.session_state.repeat_penalty = st.slider("ë°˜ë³µ í˜ë„í‹°", 0.1, 2.0, 1.1, 0.05)

    with st.expander("TTS ì„¤ì •"):
        st.session_state.tts_gender = st.selectbox("ëª©ì†Œë¦¬ ì„±ë³„", ["female", "male"], index=0)
        st.session_state.tts_speed = st.select_slider("ì½ê¸° ì†ë„", ["slow", "normal", "fast"], value="normal")

# --- ë©”ì¸ ì•± ---
st.markdown("<h1 class='title'>í•´ì•¼ í• ê¹Œìš”...? ğŸ¤”</h1>", unsafe_allow_html=True)
st.markdown("<p class='subtitle'>ì§ˆë¬¸ì„ í•˜ê³  í™•ë¥  ê¸°ë°˜ ë¶„ì„ì„ ë°›ìœ¼ì„¸ìš”!</p>", unsafe_allow_html=True)

col1, col2 = st.columns([3, 1])

with col1:
    input_type = st.radio("ì…ë ¥ ìœ í˜•", ["í…ìŠ¤íŠ¸", "ìŒì„±", "ì´ë¯¸ì§€ ì—…ë¡œë“œ", "ì‚¬ì§„ ì´¬ì˜"], horizontal=True)
    question = ""

    if input_type == "í…ìŠ¤íŠ¸":
        question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ì´ ì£¼ì‹ì„ ì‚¬ì•¼ í• ê¹Œìš”?")

    elif input_type == "ìŒì„±":
        # WebRTCë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ì˜¤ë””ì˜¤ ë…¹ìŒ (JavaScript ì‚¬ìš©)
        webrtc_ctx = webrtc_streamer(
            key="speech-recording",
            mode=WebRtcMode.SENDONLY,
            audio_receiver_size=1024,
            media_stream_constraints={"video": False, "audio": True},
        )

        if webrtc_ctx.state.playing:
            st.write("ğŸ¤ ë…¹ìŒ ì¤‘...")
        if webrtc_ctx.audio_receiver:

             # ì˜¤ë””ì˜¤ í”„ë ˆì„ì„ JavaScriptì—ì„œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì  (Python ì½œë°± ìµœì†Œí™”)
            if webrtc_ctx.audio_receiver.frames_queue.qsize() > 0:
                try:
                    audio_frames = webrtc_ctx.audio_receiver.get_frames()

                    if audio_frames:
                        latest_frame = audio_frames[-1]
                        audio_bytes = latest_frame.to_ndarray().tobytes()

                        #  JavaScriptë¡œ ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬
                        st.session_state.recorded_audio = audio_bytes # ì„¸ì…˜ì— ì €ì¥.

                except Exception as e:
                    st.error(f"Error processing audio frames: {e}")

    if input_type == "ì´ë¯¸ì§€ ì—…ë¡œë“œ":
        question = st.text_input("ì§ˆë¬¸ (ì„ íƒ ì‚¬í•­):", placeholder="ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒ ì‚¬í•­).")
        uploaded_image = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"], label_visibility='collapsed')
        camera_image = None
    elif input_type == "ì‚¬ì§„ ì´¬ì˜":
        question = st.text_input("ì§ˆë¬¸ (ì„ íƒ ì‚¬í•­):", placeholder="ì‚¬ì§„ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒ ì‚¬í•­).")
        camera_image = st.camera_input("ì‚¬ì§„ ì´¬ì˜", label_visibility="collapsed")
        uploaded_image = None
    else:
        uploaded_image = None
        camera_image = None

    # JavaScriptë¥¼ ì‚¬ìš©í•˜ì—¬ Speech Recognition API í˜¸ì¶œ (ë¸Œë¼ìš°ì € ë‚´ì—ì„œ)
    if input_type == "ìŒì„±" and st.session_state.recorded_audio:
        js_code = f"""
        async function transcribeAudio() {{
            const audioBlob = new Blob([new Uint8Array(JSON.parse('{json.dumps(list(st.session_state.recorded_audio))}'))], {{ type: 'audio/webm' }}); // ë°”ì´íŠ¸ ë°°ì—´ì„ Blobìœ¼ë¡œ
            const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
            recognition.lang = '{language_code}';
            recognition.continuous = false;
            recognition.interimResults = false;

            recognition.onresult = (event) => {{
                const transcript = event.results[0][0].transcript;
                console.log('Transcript:', transcript);
                // Streamlitì— í…ìŠ¤íŠ¸ ì „ì†¡ (callback ì‚¬ìš© X - Python ë³€ìˆ˜ì— ì§ì ‘ í• ë‹¹)
                document.dispatchEvent(new CustomEvent("set_question", {{detail: transcript}}));

            }};

            recognition.onerror = (event) => {{
                console.error('Recognition error:', event.error);
                 document.dispatchEvent(new CustomEvent("st_error", {{detail: event.error}}));

            }};
            recognition.onend = () => {{
                console.log('Recognition ended.');

            }}

            const stream = await navigator.mediaDevices.getUserMedia({{ audio: true }}); // Stream ë¶ˆí•„ìš”.
            recognition.start();


        }}
        transcribeAudio();
        """
        st.components.v1.html(f"<script>{js_code}</script>", height=0)


    # Streamlit ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ (JavaScriptì—ì„œ Pythonìœ¼ë¡œ ë°ì´í„° ì „ì†¡)
    st.markdown(
        """
        <script>
        document.addEventListener('set_question', function(e) {
            var input = window.parent.document.querySelectorAll('input[type=text]')[0];
            input.value = e.detail;
            input.dispatchEvent(new Event('input')); // Streamlitì— input ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±°

        });
         document.addEventListener('st_error', function(e) {
             // Streamlit python console ì— ë¡œê·¸
            console.error('JS STT:', e.detail);

        });

        </script>
        """,
        unsafe_allow_html=True,
    )


    if st.button("ë¶„ì„", type="primary", use_container_width=True):
        if input_type == "ì´ë¯¸ì§€ ì—…ë¡œë“œ" and not uploaded_image:
            st.warning("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        elif input_type == "ì‚¬ì§„ ì´¬ì˜" and not camera_image:
            st.warning("ì‚¬ì§„ì„ ì´¬ì˜í•´ì£¼ì„¸ìš”.")
        elif not question and input_type in ("í…ìŠ¤íŠ¸", "ìŒì„±"):
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜, ìŒì„±ì„ ë…¹ìŒí•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("ë¶„ì„ ì¤‘..."):
                if input_type in ("í…ìŠ¤íŠ¸", "ìŒì„±") and question:
                    probability, reason, audio = analyze_text(question, st.session_state.language)
                elif input_type == "ì´ë¯¸ì§€ ì—…ë¡œë“œ" and uploaded_image:
                    image_bytes = uploaded_image.getvalue()
                    image_base64 = encode_image(image_bytes)
                    probability, reason, audio = analyze_image(image_base64, question, st.session_state.language)
                elif input_type == "ì‚¬ì§„ ì´¬ì˜" and camera_image:
                    image_bytes = camera_image.getvalue()
                    image_base64 = encode_image(image_bytes)
                    probability, reason, audio = analyze_image(image_base64, question, st.session_state.language)
                else:
                    probability, reason, audio = None, "ì…ë ¥ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None

                with col2:
                    if probability is not None and reason:
                        st.subheader("ë¶„ì„ ê²°ê³¼")
                        with st.container(border=True):
                            if probability >= 50:
                                st.success(f"âœ… ì˜ˆ! ({probability}%)")
                            else:
                                st.error(f"âŒ ì•„ë‹ˆì˜¤! ({probability}%)")
                            st.progress(probability / 100.0)
                            with st.expander("ì´ìœ ", expanded=True):
                                st.markdown(reason)
                            if audio:
                                st.audio(audio, format="audio/mp3")
                    elif reason:
                        st.error(reason)
```

ì£¼ìš” ë³€ê²½ ì‚¬í•­ê³¼ ì„¤ëª…:

*   **`streamlit-webrtc`:**  `streamlit-webrtc`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ì˜¤ë””ì˜¤ë¥¼ ë…¹ìŒí•©ë‹ˆë‹¤.  ë³„ë„ì˜ Python STT ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì˜ˆ: `speech_recognition`)ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
*   **JavaScript Speech Recognition API:**  `webrtc_streamer`ë¡œ ë…¹ìŒëœ ì˜¤ë””ì˜¤ë¥¼ *ë¸Œë¼ìš°ì € ë‚´ì—ì„œ* JavaScriptì˜ `SpeechRecognition` APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.  ì´ë ‡ê²Œ í•˜ë©´ Pythonìœ¼ë¡œ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì „ì†¡í•˜ê³  ì™¸ë¶€ APIë¥¼ í˜¸ì¶œí•˜ëŠ” ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    *   `st.components.v1.html`ì„ ì‚¬ìš©í•˜ì—¬ JavaScript ì½”ë“œë¥¼ ì‚½ì…í•©ë‹ˆë‹¤.
    *   `recognition.lang`ì„ ì‚¬ìš©í•˜ì—¬ ì–¸ì–´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    *   `recognition.onresult` ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ì—ì„œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    *   `CustomEvent`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ Streamlitì˜ Python ìª½ìœ¼ë¡œ ë‹¤ì‹œ ë³´ëƒ…ë‹ˆë‹¤.  ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ (`document.addEventListener`)ë¥¼ ì‚¬ìš©í•˜ì—¬ Pythonì—ì„œ ì´ë²¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê³  `question` ë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.  ë” ì´ìƒ Streamlitì˜ ì½œë°±ì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë” íš¨ìœ¨ì ì…ë‹ˆë‹¤.
*   **ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬:** ì˜¤ë””ì˜¤ í”„ë ˆì„ì€ JavaScriptì—ì„œ ì§ì ‘ `Blob` ê°ì²´ë¡œ ë³€í™˜ë˜ì–´ ì²˜ë¦¬ë©ë‹ˆë‹¤.  ì´ë ‡ê²Œí•˜ë©´ Python ì½œë°±ì„ ìµœì†Œí™”í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
* **ì„¸ì…˜ ìƒíƒœ (`st.session_state.recorded_audio`):** ë…¹ìŒëœ ì˜¤ë””ì˜¤ ë°ì´í„°(ë°”ì´íŠ¸)ëŠ” ì„¸ì…˜ ìƒíƒœì— ì €ì¥ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ í˜ì´ì§€ ìƒˆë¡œ ê³ ì¹¨ í›„ì—ë„ ìœ ì§€ë©ë‹ˆë‹¤.
*   **ì˜¤ë¥˜ ì²˜ë¦¬:**  JavaScript `recognition.onerror` ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„± ì¸ì‹ ì˜¤ë¥˜ë¥¼ ì²˜ë¦¬í•˜ê³ , ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ Streamlit Python ì½˜ì†”ì— í‘œì‹œí•©ë‹ˆë‹¤.
*   **gTTS ìœ ì§€:**  gTTSëŠ” ì—¬ì „íˆ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.  ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ TTSë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì€ (Web Speech APIë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ë„) gTTSë§Œí¼ ìœ ì—°í•˜ê±°ë‚˜ ë‹¤ì–‘í•œ ì–¸ì–´/ìŒì„±ì„ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ë§Œì•½ ë” ë‹¤ì–‘í•œ ìŒì„± ì˜µì…˜ì´ í•„ìš”í•˜ë‹¤ë©´, Google Cloud Text-to-Speechì™€ ê°™ì€ í´ë¼ìš°ë“œ ê¸°ë°˜ TTS APIë¥¼ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
*   **`requirements.txt` ë‹¨ìˆœí™”:**  ë” ì´ìƒ `google-cloud-speech` (ë˜ëŠ” ë‹¤ë¥¸ STT API íŒ¨í‚¤ì§€)ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. `requirements.txt`ì—ëŠ” `streamlit`, `requests`, `gTTS`, `duckduckgo-search`, `streamlit-webrtc`ë§Œ ìˆìœ¼ë©´ ë©ë‹ˆë‹¤.

ì´ ë²„ì „ì€ ëŒ€ë¶€ë¶„ì˜ ê¸°ëŠ¥ì„ ì›¹ ë¸Œë¼ìš°ì € ë‚´ì—ì„œ ì²˜ë¦¬í•˜ì—¬ ì™¸ë¶€ API ì˜ì¡´ì„±ì„ ìµœì†Œí™”í•˜ê³ , í•˜ë‚˜ì˜ `app.py` íŒŒì¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.  ë‹¤ë§Œ, TTSëŠ” ì—¬ì „íˆ gTTSë¥¼ ì‚¬ìš©í•˜ë©°, ì´ëŠ” Pythonì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.  ì™„ë²½í•˜ê²Œ ë¸Œë¼ìš°ì € ê¸°ë°˜ì˜ TTSë¥¼ ì›í•œë‹¤ë©´ Web Speech APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ, gTTSë§Œí¼ ë‹¤ì–‘í•œ ì–¸ì–´ì™€ ìŒì„±ì„ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ìµœìƒì˜ ì‚¬ìš©ì ê²½í—˜ì„ ìœ„í•´ì„œëŠ” ì—¬ì „íˆ í´ë¼ìš°ë“œ ê¸°ë°˜ TTS API (ì˜ˆ: Google Cloud Text-to-Speech)ë¥¼ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

ì´ ì½”ë“œëŠ” `streamlit run app.py`ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `streamlit-webrtc`ëŠ” ë¡œì»¬ í™˜ê²½(`localhost`)ì—ì„œëŠ” HTTPë¡œ ì‘ë™í•˜ì§€ë§Œ, ë°°í¬ ì‹œì—ëŠ” HTTPSê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**í•µì‹¬ ë³€ê²½ ì‚¬í•­:**

1.  **`speech_recognition` ì œê±°, `streamlit-webrtc` ë° STT API ì‚¬ìš©:**
    *   `speech_recognition` ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ PyAudio ì˜ì¡´ì„±ì„ ì œê±°í•©ë‹ˆë‹¤.  ì´ë¡œì¨ ì„¤ì¹˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ë” ì•ˆì •ì ì¸ ì›¹ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    *   `streamlit-webrtc`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ì˜¤ë””ì˜¤ë¥¼ ë…¹ìŒí•©ë‹ˆë‹¤.  ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” Streamlitê³¼ ì˜ í†µí•©ë˜ë©° ë³„ë„ì˜ ì„œë²„ ì„¤ì • ì—†ì´ ì‘ë™í•©ë‹ˆë‹¤.
    *   ë…¹ìŒëœ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ Google Cloud Speech-to-Text API, AssemblyAI, Whisper API ë“± *ìƒìš© STT API*ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.  (ì˜ˆì œì—ì„œëŠ” Google Cloud STTë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë‹¤ë¥¸ APIë¡œ ì‰½ê²Œ êµì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)  ì›¹ APIë¥¼ ì‚¬ìš©í•˜ë©´ ë” ë†’ì€ ì •í™•ë„ì™€ ì•ˆì •ì„±ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        *  Google Cloud STTë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ Google Cloud Platform (GCP) í”„ë¡œì íŠ¸ë¥¼ ìƒì„±í•˜ê³ , Speech-to-Text APIë¥¼ í™œì„±í™”í•˜ê³ , ì„œë¹„ìŠ¤ ê³„ì • í‚¤ (JSON íŒŒì¼)ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.  ì´ í‚¤ íŒŒì¼ì„ í”„ë¡œì íŠ¸ì— ì¶”ê°€í•˜ê³  í™˜ê²½ ë³€ìˆ˜ `GOOGLE_APPLICATION_CREDENTIALS`ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.

2.  **gTTS ì„¤ì • í™•ì¥:**
    *   ì„±ë³„ (ë‚¨ì„±/ì—¬ì„±) ë° ì†ë„ (ëŠë¦¼/ë³´í†µ/ë¹ ë¦„)ë¥¼ ì„ íƒí•  ìˆ˜ ìˆëŠ” UI ìš”ì†Œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    *   gTTS ê°ì²´ë¥¼ ìƒì„±í•  ë•Œ ì´ëŸ¬í•œ ì„¤ì •ì„ ë°˜ì˜í•©ë‹ˆë‹¤.

3.  **LLM ì„¤ì • í™•ì¥:**
    *   `top_p`, `top_k`, `repeat_penalty`ì™€ ê°™ì€ ì¶”ê°€ì ì¸ LLM íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆëŠ” UI ìš”ì†Œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    *   `call_ollama_api` í•¨ìˆ˜ì—ì„œ ì´ëŸ¬í•œ íŒŒë¼ë¯¸í„°ë¥¼ `options`ì— í¬í•¨ì‹œì¼œ Ollama APIì— ì „ë‹¬í•©ë‹ˆë‹¤.

4.  **ì½”ë“œ ì •ë¦¬ ë° ëª¨ë“ˆí™”:**
    *   STT, TTS ê´€ë ¨ í•¨ìˆ˜ë¥¼ ë³„ë„ì˜ ëª¨ë“ˆ (`stt_tts.py`)ë¡œ ë¶„ë¦¬í•˜ì—¬ ì½”ë“œë¥¼ ë” ê¹”ë”í•˜ê²Œ êµ¬ì„±í•©ë‹ˆë‹¤.
    *   `utils.py` ì— ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë  í•¨ìˆ˜ë¥¼ ë„£ì–´ë‘ .

**ì „ì²´ ì½”ë“œ (êµ¬ì¡° ë³€ê²½):**

ë‹¤ìŒì€ í”„ë¡œì íŠ¸ì˜ êµ¬ì¡°ì…ë‹ˆë‹¤.

```
should_i/
â”œâ”€â”€ app.py           # ë©”ì¸ Streamlit ì•±
â”œâ”€â”€ stt_tts.py        # STT, TTS ê´€ë ¨ í•¨ìˆ˜
â”œâ”€â”€ utils.py        # ê³µí†µ í•¨ìˆ˜
â””â”€â”€ requirements.txt  # í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª©ë¡
```

**requirements.txt:**

```
streamlit
requests
gTTS
duckduckgo-search
streamlit-webrtc
google-cloud-speech # Google Cloud STT ì‚¬ìš© ì‹œ
# assemblyai  # AssemblyAI ì‚¬ìš© ì‹œ
# openai      # Whisper API ì‚¬ìš© ì‹œ
```

**utils.py:**
```python
import base64

def encode_image(image_bytes):
    """Encodes image bytes to base64."""
    return base64.b64encode(image_bytes).decode("utf-8")
```

**stt_tts.py:**

```python
import io
import streamlit as st
from gtts import gTTS
from google.cloud import speech  # Google Cloud STT


def google_cloud_stt(audio_bytes, language_code):
    """Transcribes audio bytes using Google Cloud Speech-to-Text."""
    try:
        client = speech.SpeechClient()
        audio = speech.RecognitionAudio(content=audio_bytes)
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            # sample_rate_hertz=44100,  # Adjust if needed
            sample_rate_hertz=48000,
            language_code=language_code,
            enable_automatic_punctuation=True, # êµ¬ë‘ì  ì¶”ê°€.
        )

        response = client.recognize(config=config, audio=audio)
        if response.results:
            return response.results[0].alternatives[0].transcript
        else:
            return None

    except Exception as e:
        st.error(f"Google Cloud STT Error: {e}")
        return None

# --- ë‹¤ë¥¸ STT API ì‚¬ìš© ì˜ˆì‹œ (AssemblyAI, Whisper) ---
# def assemblyai_stt(audio_bytes, api_key):
#    # AssemblyAI API implementation (requires assemblyai package)
#     ...

# def whisper_stt(audio_bytes, api_key):
#     # OpenAI Whisper API implementation (requires openai package)
#     ...


def text_to_speech(text, language="ko", gender="female", speed="normal"):
    """Converts text to speech using gTTS with gender and speed options."""
    try:
        # gTTSì—ì„œ ì†ë„ ì¡°ì ˆì€ slow=True/Falseë¡œë§Œ ê°€ëŠ¥.  ë” ì„¸ë°€í•œ ì¡°ì ˆì€ ë‹¤ë¥¸ TTS ì—”ì§„ í•„ìš”.
        slow = speed == "slow"
        tts = gTTS(text=text, lang=language, slow=slow, tld="com") # tld ì„¤ì •

        # ì„±ë³„ ì„¤ì •ì€ gTTSì—ì„œ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ.  ë‹¤ë¥¸ TTS ì—”ì§„ (ì˜ˆ: Google Cloud Text-to-Speech) í•„ìš”.
        # ì—¬ê¸°ì„œëŠ” gTTSë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, ì„±ë³„ì€ ë¬´ì‹œë¨.  ë” ë‚˜ì€ ì„±ë³„ ì œì–´ë¥¼ ì›í•˜ë©´ ë‹¤ë¥¸ TTS API ì‚¬ìš©.

        buffer = io.BytesIO()
        tts.write_to_fp(buffer)
        buffer.seek(0)
        return buffer.read()
    except Exception as e:
        st.error(f"gTTS ì˜¤ë¥˜: {e}")
        return None

```

**app.py:**

```python
import streamlit as st
import requests
from duckduckgo_search import DDGS
from stt_tts import google_cloud_stt, text_to_speech  # Import STT and TTS functions
from utils import encode_image
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import av

# --- Constants ---
OLLAMA_HOST = "http://192.168.0.119:11434"  # Replace with your Ollama server
OLLAMA_MODEL = "llama3.2-vision"
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ì™€ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì„¸ìš”.
ë‹¤ìŒ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µì„ ì œê³µí•˜ì„¸ìš”:

- "probability": "ì˜ˆ" (í•´ì•¼ í•¨/êµ¬ë§¤í•´ì•¼ í•¨) ëŒ€ "ì•„ë‹ˆì˜¤" (í•˜ì§€ ë§ì•„ì•¼ í•¨/êµ¬ë§¤í•˜ì§€ ë§ì•„ì•¼ í•¨)ì˜ ê°€ëŠ¥ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ë°±ë¶„ìœ¨ (0-100)ì…ë‹ˆë‹¤.
                   ë” ë†’ì€ ë°±ë¶„ìœ¨ì€ "ì˜ˆ"ë¥¼ ì˜ë¯¸í•˜ê³ , ë” ë‚®ì€ ë°±ë¶„ìœ¨ì€ "ì•„ë‹ˆì˜¤"ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
- "reason": í•´ë‹¹ë˜ëŠ” ê²½ìš° ì¶œì²˜ë¥¼ ì¸ìš©í•˜ì—¬ ë¶„ì„ ë° ì¶”ë¡ ì— ëŒ€í•œ ê°„ê²°í•œ ì„¤ëª….

ì˜ˆì‹œ:
{"probability": 75, "reason": "í˜„ì¬ ì‹œì¥ ë™í–¥ê³¼ ì „ë¬¸ê°€ ì˜ê²¬ì— ë”°ë¥´ë©´, í•´ë‹¹ ì£¼ì‹ì€ ê°•ë ¥í•œ ì„±ì¥ ì ì¬ë ¥ì„ ë³´ì…ë‹ˆë‹¤."}
"""

# --- Helper Functions ---

def perform_ddg_search(query, max_results=3):
    """Performs DuckDuckGo search; handles empty queries."""
    if not query:
        return ""
    try:
        with DDGS() as ddgs:
            results = [r["body"] for r in ddgs.text(query, max_results=max_results)]
        return "\n\n".join(results)
    except Exception as e:
        st.error(f"DuckDuckGo ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return ""


def call_ollama_api(messages, stream=False, format="json"):
    """Calls the Ollama API."""
    data = {
        "model": OLLAMA_MODEL,
        "messages": messages,
        "stream": stream,
        "format": format,
        "options": {
            "temperature": st.session_state.temperature,
            "num_predict": st.session_state.max_tokens,
            "top_p": st.session_state.top_p,
            "top_k": st.session_state.top_k,
            "repeat_penalty": st.session_state.repeat_penalty,
        },
    }
    try:
        response = requests.post(f"{OLLAMA_HOST}/api/chat", json=data)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"Ollama API ì˜¤ë¥˜: {e}")
        return None



# --- Analysis Functions ---

def analyze_image(image_data, question, language):
    """Analyzes image and question using Ollama."""
    if question:
        search_results = perform_ddg_search(question, max_results=2)
        combined_input = f"{question}\n\nê´€ë ¨ ì •ë³´:\n{search_results}"
    else:
        combined_input = "ì‚¬ì§„ì— ëŒ€í•´ ë¶„ì„í•´ì£¼ì„¸ìš”."

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": combined_input, "images": [image_data]},
    ]
    response_json = call_ollama_api(messages)
    return process_ollama_response(response_json, language) if response_json else (None, "ì˜¤ë¥˜: Ollama API í˜¸ì¶œ ì‹¤íŒ¨.", None)


def analyze_text(question, language):
    """Analyzes text question using Ollama."""
    search_results = perform_ddg_search(question)
    combined_input = f"{question}\n\nê´€ë ¨ ì •ë³´:\n{search_results}"
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": combined_input},
    ]
    response_json = call_ollama_api(messages)
    return process_ollama_response(response_json, language) if response_json else (None, "ì˜¤ë¥˜: Ollama API í˜¸ì¶œ ì‹¤íŒ¨.", None)

def process_ollama_response(response_json, language):
    """Processes the Ollama API response."""
    try:
        content_str = response_json['message']['content']
        content_json = json.loads(content_str)
        probability = content_json.get("probability", None)
        reason = content_json.get("reason", None)

        if probability is not None and not (0 <= probability <= 100):
            raise ValueError("í™•ë¥ ì´ 0-100 ë²”ìœ„ ë‚´ì— ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

        audio_bytes = text_to_speech(
            reason, language, st.session_state.tts_gender, st.session_state.tts_speed
        ) if reason else None

        return probability, reason, audio_bytes
    except (json.JSONDecodeError, KeyError, ValueError) as e:
        st.error(f"Ollama ì‘ë‹µ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None, "ì˜¤ë¥˜: Ollamaë¡œë¶€í„° ìœ íš¨í•˜ì§€ ì•Šì€ ì‘ë‹µ.", None


# --- Streamlit App ---

st.set_page_config(page_title="í•´ì•¼ í• ê¹Œìš”...?", page_icon="ğŸ¤”", layout="wide")

# --- Custom CSS ---
st.markdown("""
<style>
/* Main container */
.main {
    padding: 2rem;
    font-family: sans-serif;
}

/* Title */
.title {
    font-size: 3rem;
    font-weight: bold;
    margin-bottom: 1rem;
    color: #333;
}

/* Subtitle/Description */
.subtitle {
    font-size: 1.25rem;
    color: #555;
    margin-bottom: 2rem;
}

/* Input area */
.stTextInput, .stFileUploader, .stCameraInput, .stRadio {
    margin-bottom: 1rem;
}

/* Buttons */
.stButton>button {
    background-color: #007bff;
    color: white;
    border: none;
    padding: 0.75rem 1.5rem;
    border-radius: 0.5rem;
    font-weight: bold;
    transition: background-color 0.2s ease;
}
.stButton>button:hover {
    background-color: #0056b3;
}
.stButton>button:focus {
    outline: none;
    box-shadow: 0 0 0 3px rgba(0, 123, 255, 0.25);
}

/* Radio buttons */
.stRadio > div > label {
  margin-right: 1rem;
  padding: 0.5rem 1rem;
  border-radius: 0.5rem;
  border: 1px solid #ccc;
  transition: all 0.2s ease;
}
.stRadio > div > label[data-baseweb="radio"] > div:first-child {
    background-color: #007bff !important;
    border-color: #007bff !important;
}
.stRadio > div > label[data-baseweb="radio"] {
    background-color: #f0f2f6;
    border-color: #f0f2f6;
}

/* Result area */
.result-area {
    padding: 1.5rem;
    border-radius: 0.75rem;
    background-color: #f8f9fa;
    border: 1px solid #e9ecef;
}

/* Success/Error messages */
.st-success, .st-error {
    padding: 1rem;
    border-radius: 0.5rem;
    margin-bottom: 1rem;
    font-weight: bold;
    color: white;
    display: flex;
    align-items: center;
}
.st-success {
    background-color: #28a745;
}
.st-error {
    background-color: #dc3545;
}
.st-success svg, .st-error svg {
    margin-right: 0.5rem;
    font-size: 1.2em;
}

/* Expander */
.st-expander {
  border: none !important;
}
.st-expanderHeader {
    font-weight: bold;
    background-color: transparent;
    padding: 0.5rem 0;
}
.st-expanderContent {
    padding: 0.5rem 0;
}

/* Gauge/Progress */
.stProgress > div > div > div > div {
    background-color: #007bff;
}

/* Sidebar */
.sidebar .sidebar-content {
    padding: 1rem;
    background-color: #f0f2f6;
}
</style>
""", unsafe_allow_html=True)

# --- Session State ---
if "language" not in st.session_state:
    st.session_state["language"] = "ko"
if "max_tokens" not in st.session_state:
    st.session_state["max_tokens"] = 256
if "temperature" not in st.session_state:
    st.session_state["temperature"] = 0.7
if "top_p" not in st.session_state:
    st.session_state["top_p"] = 0.9
if "top_k" not in st.session_state:
    st.session_state["top_k"] = 50
if "repeat_penalty" not in st.session_state:
    st.session_state["repeat_penalty"] = 1.1
if "tts_gender" not in st.session_state:  # TTS ì„¤ì •
    st.session_state["tts_gender"] = "female"
if "tts_speed" not in st.session_state:
    st.session_state["tts_speed"] = "normal"



# --- Sidebar ---
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    st.session_state.language = st.selectbox("ì–¸ì–´", ["ko", "en", "ja", "zh-CN", "fr", "de"], index=0)
    language_code = st.session_state.language.split("-")[0]

    with st.expander("LLM ì„¤ì •"):
        st.session_state.max_tokens = st.slider("ìµœëŒ€ í† í° ìˆ˜", 1, 2048, 256, 1)
        st.session_state.temperature = st.slider("ì˜¨ë„", 0.1, 4.0, 0.7, 0.1)
        st.session_state.top_p = st.slider("Top P", 0.1, 1.0, 0.9, 0.05)
        st.session_state.top_k = st.slider("Top K", 1, 100, 50, 1)
        st.session_state.repeat_penalty = st.slider("ë°˜ë³µ í˜ë„í‹°", 0.1, 2.0, 1.1, 0.05)

    with st.expander("TTS ì„¤ì •"):
        st.session_state.tts_gender = st.selectbox("ëª©ì†Œë¦¬ ì„±ë³„", ["female", "male"], index=0)  # gTTSëŠ” ì œí•œì 
        st.session_state.tts_speed = st.select_slider("ì½ê¸° ì†ë„", ["slow", "normal", "fast"], value="normal")


# --- Main App ---
st.markdown("<h1 class='title'>í•´ì•¼ í• ê¹Œìš”...? ğŸ¤”</h1>", unsafe_allow_html=True)
st.markdown("<p class='subtitle'>ì§ˆë¬¸ì„ í•˜ê³  í™•ë¥  ê¸°ë°˜ ë¶„ì„ì„ ë°›ìœ¼ì„¸ìš”!</p>", unsafe_allow_html=True)

col1, col2 = st.columns([3, 1])

with col1:
    input_type = st.radio("ì…ë ¥ ìœ í˜•", ["í…ìŠ¤íŠ¸", "ìŒì„±", "ì´ë¯¸ì§€ ì—…ë¡œë“œ", "ì‚¬ì§„ ì´¬ì˜"], horizontal=True)
    question = ""
    audio_bytes = None # ìŒì„± ë°”ì´íŠ¸

    if input_type == "í…ìŠ¤íŠ¸":
        question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ì´ ì£¼ì‹ì„ ì‚¬ì•¼ í• ê¹Œìš”?")

    elif input_type == "ìŒì„±":
        # webrtc_streamerë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ë…¹ìŒ
        webrtc_ctx = webrtc_streamer(
            key="speech-recording",
            mode=WebRtcMode.SENDONLY,
            audio_receiver_size=1024,
            # rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}, # í•„ìš” ì‹œ STUN ì„œë²„ ì„¤ì •
            media_stream_constraints={"video": False, "audio": True},
        )

        if webrtc_ctx.state.playing:
            st.write("ğŸ¤ ë…¹ìŒ ì¤‘...")

        if webrtc_ctx.audio_receiver:
            try:
                audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=1)
                if audio_frames:
                    # ìµœì‹  í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
                    latest_frame = audio_frames[-1]
                    audio_bytes = latest_frame.to_ndarray().tobytes()


                    if audio_bytes:
                        question = google_cloud_stt(audio_bytes, language_code) # STT
                        if question:
                            st.write("ë‹¹ì‹ ì˜ ì§ˆë¬¸:", question)
            except Exception as e:
                 st.error(f"ì˜¤ë¥˜: {e}")

    if input_type == "ì´ë¯¸ì§€ ì—…ë¡œë“œ":
        question = st.text_input("ì§ˆë¬¸ (ì„ íƒ ì‚¬í•­):", placeholder="ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒ ì‚¬í•­).")
        uploaded_image = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"], label_visibility='collapsed')
        camera_image = None
    elif input_type == "ì‚¬ì§„ ì´¬ì˜":
        question = st.text_input("ì§ˆë¬¸ (ì„ íƒ ì‚¬í•­):", placeholder="ì‚¬ì§„ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒ ì‚¬í•­).")
        camera_image = st.camera_input("ì‚¬ì§„ ì´¬ì˜", label_visibility="collapsed")
        uploaded_image = None
    else:
        uploaded_image = None
        camera_image = None


    if st.button("ë¶„ì„", type="primary", use_container_width=True):
        if input_type == "ì´ë¯¸ì§€ ì—…ë¡œë“œ" and not uploaded_image:
             st.warning("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        elif input_type == "ì‚¬ì§„ ì´¬ì˜" and not camera_image:
            st.warning("ì‚¬ì§„ì„ ì´¬ì˜í•´ì£¼ì„¸ìš”")
        elif not question and input_type in ("í…ìŠ¤íŠ¸", "ìŒì„±") :
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜, ìŒì„±ì„ ë…¹ìŒí•´ì£¼ì„¸ìš”.")

        else:
            with st.spinner("ë¶„ì„ ì¤‘..."):
                if input_type in ("í…ìŠ¤íŠ¸", "ìŒì„±") and question:
                    probability, reason, audio = analyze_text(question, st.session_state.language)
                elif input_type == "ì´ë¯¸ì§€ ì—…ë¡œë“œ" and uploaded_image:
                    image_bytes = uploaded_image.getvalue()
                    image_base64 = encode_image(image_bytes)
                    probability, reason, audio = analyze_image(image_base64, question, st.session_state.language)
                elif input_type == "ì‚¬ì§„ ì´¬ì˜" and camera_image:
                    image_bytes = camera_image.getvalue()
                    image_base64 = encode_image(image_bytes)
                    probability, reason, audio = analyze_image(image_base64, question, st.session_state.language)

                else:
                    probability, reason, audio = None, "ì…ë ¥ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None

                with col2:
                    if probability is not None and reason:
                        st.subheader("ë¶„ì„ ê²°ê³¼")
                        with st.container(border=True):
                            if probability >= 50:
                                st.success(f"âœ… ì˜ˆ! ({probability}%)")
                            else:
                                st.error(f"âŒ ì•„ë‹ˆì˜¤! ({probability}%)")
                            st.progress(probability / 100.0)
                            with st.expander("ì´ìœ ", expanded=True):
                                st.markdown(reason)
                            if audio:
                                st.audio(audio, format="audio/mp3")
                    elif reason:
                        st.error(reason)
```

**ì‹¤í–‰ ë°©ë²• ë° ì£¼ì˜ ì‚¬í•­:**

1.  **í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜:** `pip install -r requirements.txt`
2.  **Google Cloud Speech-to-Text ì„¤ì • (Google Cloud STT ì‚¬ìš© ì‹œ):**
    *   GCP í”„ë¡œì íŠ¸ ìƒì„± ë° Speech-to-Text API í™œì„±í™”.
    *   ì„œë¹„ìŠ¤ ê³„ì • í‚¤ (JSON íŒŒì¼) ìƒì„±.
    *   `GOOGLE_APPLICATION_CREDENTIALS` í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ê²½ë¡œë¡œ ì„¤ì •.  (ë˜ëŠ” ì½”ë“œ ë‚´ì—ì„œ `os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "path/to/your/key.json"`ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ, ë³´ì•ˆìƒ ê¶Œì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)
3.  **Ollama ì„œë²„ ì‹¤í–‰:** Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ê³  `OLLAMA_HOST`ì— ì˜¬ë°”ë¥¸ ì£¼ì†Œê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
4.  **Streamlit ì•± ì‹¤í–‰:** `streamlit run app.py`
5.  **HTTPS (ê¶Œì¥):** `streamlit-webrtc`ëŠ” ë¡œì»¬ í™˜ê²½ (`localhost`)ì—ì„œëŠ” HTTPë¡œ ì‘ë™í•˜ì§€ë§Œ, ë°°í¬ ì‹œì—ëŠ” HTTPSê°€ í•„ìš”í•©ë‹ˆë‹¤.  HTTPSë¥¼ ì‚¬ìš©í•˜ë©´ ë¸Œë¼ìš°ì €ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ ë” ì‰½ê²Œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ì œ ì•±ì€ ì›¹ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„± ì…ë ¥ì„ ì²˜ë¦¬í•˜ê³ , gTTS ë° LLM ì„¤ì •ì´ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.  Google Cloud STTë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  ë‹¤ë¥¸ STT API (AssemblyAI, Whisper API ë“±)ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `stt_tts.py` íŒŒì¼ì—ì„œ í•´ë‹¹ APIë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤.
