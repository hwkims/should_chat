import streamlit as st
import requests
import base64
import json
import io
from gtts import gTTS  # gTTSëŠ” ì—¬ì „íˆ ì‚¬ìš© (ì›¹ API ëŒ€ì•ˆì´ ì œí•œì )
from duckduckgo_search import DDGS
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import av
import logging

# ë¡œê¹… ì„¤ì • (webrtc ê´€ë ¨ ì˜¤ë¥˜ë¥¼ ë³´ê¸° ìœ„í•¨)
logging.basicConfig(level=logging.DEBUG)

# --- ìƒìˆ˜ ---
OLLAMA_HOST = "http://192.168.0.119:11434"  # Ollama ì„œë²„ ì£¼ì†Œ
OLLAMA_MODEL = "llama3.2-vision"
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ì™€ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì„¸ìš”.
ë‹¤ìŒ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µì„ ì œê³µí•˜ì„¸ìš”:

- "probability": "ì˜ˆ" (í•´ì•¼ í•¨/êµ¬ë§¤í•´ì•¼ í•¨) ëŒ€ "ì•„ë‹ˆì˜¤" (í•˜ì§€ ë§ì•„ì•¼ í•¨/êµ¬ë§¤í•˜ì§€ ë§ì•„ì•¼ í•¨)ì˜ ê°€ëŠ¥ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ë°±ë¶„ìœ¨ (0-100)ì…ë‹ˆë‹¤.
                   ë” ë†’ì€ ë°±ë¶„ìœ¨ì€ "ì˜ˆ"ë¥¼ ì˜ë¯¸í•˜ê³ , ë” ë‚®ì€ ë°±ë¶„ìœ¨ì€ "ì•„ë‹ˆì˜¤"ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
- "reason": í•´ë‹¹ë˜ëŠ” ê²½ìš° ì¶œì²˜ë¥¼ ì¸ìš©í•˜ì—¬ ë¶„ì„ ë° ì¶”ë¡ ì— ëŒ€í•œ ê°„ê²°í•œ ì„¤ëª….

ì˜ˆì‹œ:
{"probability": 75, "reason": "í˜„ì¬ ì‹œì¥ ë™í–¥ê³¼ ì „ë¬¸ê°€ ì˜ê²¬ì— ë”°ë¥´ë©´, í•´ë‹¹ ì£¼ì‹ì€ ê°•ë ¥í•œ ì„±ì¥ ì ì¬ë ¥ì„ ë³´ì…ë‹ˆë‹¤."}
"""

# --- ë„ìš°ë¯¸ í•¨ìˆ˜ ---

def text_to_speech(text, language="ko", gender="female", speed="normal"):
    """gTTSë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤ (ì„±ë³„, ì†ë„ ì˜µì…˜ í¬í•¨)."""
    try:
        slow = speed == "slow"
        tts = gTTS(text=text, lang=language, slow=slow, tld="com") # tld ì„¤ì •.

        # ì„±ë³„ ì„¤ì •ì€ gTTSì—ì„œ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ. ë‹¤ë¥¸ TTSì—”ì§„ í•„ìš”.

        buffer = io.BytesIO()
        tts.write_to_fp(buffer)
        buffer.seek(0)
        return buffer.read()
    except Exception as e:
        st.error(f"gTTS ì˜¤ë¥˜: {e}")
        return None

def encode_image(image_bytes):
    """ì´ë¯¸ì§€ ë°”ì´íŠ¸ë¥¼ base64ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤."""
    return base64.b64encode(image_bytes).decode("utf-8")

def perform_ddg_search(query, max_results=3):
    """DuckDuckGo ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ì—°ê²°ëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (ë¹ˆ ì¿¼ë¦¬ ì²˜ë¦¬)."""
    if not query:
        return ""
    try:
        with DDGS() as ddgs:
            results = [r["body"] for r in ddgs.text(query, max_results=max_results)]
        return "\n\n".join(results)
    except Exception as e:
        st.error(f"DuckDuckGo ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return ""

def call_ollama_api(messages, stream=False, format="json"):
    """Ollama APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤ (í™•ì¥ëœ LLM ì˜µì…˜ í¬í•¨)."""
    data = {
        "model": OLLAMA_MODEL,
        "messages": messages,
        "stream": stream,
        "format": format,
        "options": {
            "temperature": st.session_state.temperature,
            "num_predict": st.session_state.max_tokens,
            "top_p": st.session_state.top_p,
            "top_k": st.session_state.top_k,
            "repeat_penalty": st.session_state.repeat_penalty,
        },
    }
    try:
        response = requests.post(f"{OLLAMA_HOST}/api/chat", json=data)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"Ollama API ì˜¤ë¥˜: {e}")
        return None

# --- ë¶„ì„ í•¨ìˆ˜ ---
def analyze_image(image_data, question, language):
    """ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì„ ë¶„ì„í•©ë‹ˆë‹¤ (Ollama ì‚¬ìš©, ì„ íƒì  DuckDuckGo ê²€ìƒ‰)."""
    if question:
        search_results = perform_ddg_search(question, max_results=2)
        combined_input = f"{question}\n\nê´€ë ¨ ì •ë³´:\n{search_results}"
    else:
        combined_input = "ì‚¬ì§„ì— ëŒ€í•´ ë¶„ì„í•´ì£¼ì„¸ìš”."

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": combined_input, "images": [image_data]},
    ]
    response_json = call_ollama_api(messages)
    return process_ollama_response(response_json, language) if response_json else (None, "ì˜¤ë¥˜: Ollama API í˜¸ì¶œ ì‹¤íŒ¨.", None)

def analyze_text(question, language):
    """í…ìŠ¤íŠ¸ ì§ˆë¬¸ì„ ë¶„ì„í•©ë‹ˆë‹¤ (Ollama ì‚¬ìš©, DuckDuckGo ê²€ìƒ‰)."""
    search_results = perform_ddg_search(question)
    combined_input = f"{question}\n\nê´€ë ¨ ì •ë³´:\n{search_results}"
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": combined_input},
    ]
    response_json = call_ollama_api(messages)
    return process_ollama_response(response_json, language) if response_json else (None, "ì˜¤ë¥˜: Ollama API í˜¸ì¶œ ì‹¤íŒ¨.", None)

def process_ollama_response(response_json, language):
    """Ollama API ì‘ë‹µì„ ì²˜ë¦¬í•˜ê³ , ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³ , TTSë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        content_str = response_json['message']['content']
        content_json = json.loads(content_str)
        probability = content_json.get("probability", None)
        reason = content_json.get("reason", None)

        if probability is not None and not (0 <= probability <= 100):
            raise ValueError("í™•ë¥ ì´ 0-100 ë²”ìœ„ ë‚´ì— ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # ì„ íƒëœ ì„±ë³„ê³¼ ì†ë„ë¡œ TTS ìƒì„±
        audio_bytes = text_to_speech(
            reason, language, st.session_state.tts_gender, st.session_state.tts_speed
        ) if reason else None

        return probability, reason, audio_bytes
    except (json.JSONDecodeError, KeyError, ValueError) as e:
        st.error(f"Ollama ì‘ë‹µ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None, "ì˜¤ë¥˜: Ollamaë¡œë¶€í„° ìœ íš¨í•˜ì§€ ì•Šì€ ì‘ë‹µ.", None

# --- Streamlit ì•± ---

st.set_page_config(page_title="í•´ì•¼ í• ê¹Œìš”...?", page_icon="ğŸ¤”", layout="wide")

# --- Custom CSS ---
st.markdown("""
<style>
/* (ì´ì „ CSS ìŠ¤íƒ€ì¼) */
/* Main container */
.main {
    padding: 2rem;
    font-family: sans-serif; /* Modern font */
}

/* Title */
.title {
    font-size: 3rem;
    font-weight: bold;
    margin-bottom: 1rem;
    color: #333;  /* Darker title color */
}

/* Subtitle/Description */
.subtitle {
    font-size: 1.25rem;
    color: #555;
    margin-bottom: 2rem;
}

/* Input area */
.stTextInput, .stFileUploader, .stCameraInput, .stRadio {
    margin-bottom: 1rem;
}

/* Buttons */
.stButton>button {
    background-color: #007bff; /* Primary blue color */
    color: white;
    border: none;
    padding: 0.75rem 1.5rem;
    border-radius: 0.5rem; /* Rounded corners */
    font-weight: bold;
    transition: background-color 0.2s ease; /* Smooth hover effect */
}
.stButton>button:hover {
    background-color: #0056b3; /* Darker blue on hover */
}
.stButton>button:focus {
    outline: none;  /* Remove the focus outline */
    box-shadow: 0 0 0 3px rgba(0, 123, 255, 0.25); /* Add a subtle focus ring */
}

/* Radio buttons */
.stRadio > div > label { /* Target radio button labels */
  margin-right: 1rem; /* Add spacing */
  padding: 0.5rem 1rem;
  border-radius: 0.5rem; /* Rounded */
  border: 1px solid #ccc;
  transition: all 0.2s ease;  /* Smooth transition */
}
/* Style the selected radio option */
.stRadio > div > label[data-baseweb="radio"] > div:first-child {
    background-color: #007bff !important;  /* Override to ensure color */
    border-color: #007bff !important;
}
.stRadio > div > label[data-baseweb="radio"] {
    background-color: #f0f2f6;
    border-color: #f0f2f6;
}


/* Result area */
.result-area {
    padding: 1.5rem;
    border-radius: 0.75rem;
    background-color: #f8f9fa; /* Light background */
    border: 1px solid #e9ecef;  /* Subtle border */
}

/* Success/Error messages */
.st-success, .st-error {
    padding: 1rem;
    border-radius: 0.5rem;
    margin-bottom: 1rem;
    font-weight: bold;
    color: white;
    display: flex;
    align-items: center;
}
.st-success {
    background-color: #28a745; /* Green */
}
.st-error {
    background-color: #dc3545; /* Red */
}
.st-success svg, .st-error svg { /* Icons within success/error */
    margin-right: 0.5rem;  /* Space the icon */
    font-size: 1.2em;
}


/* Expander */
.st-expander {
  border: none !important; /* No borders */
}
.st-expanderHeader {
    font-weight: bold;
    background-color: transparent;
    padding: 0.5rem 0;
}
.st-expanderContent {
    padding: 0.5rem 0;
}

/* Gauge/Progress */
.stProgress > div > div > div > div {
    background-color: #007bff; /* Consistent blue */
}

/* Sidebar */
.sidebar .sidebar-content {
    padding: 1rem;
    background-color: #f0f2f6; /* Lighter background */
}
</style>
""", unsafe_allow_html=True)

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "language" not in st.session_state:
    st.session_state["language"] = "ko"
if "max_tokens" not in st.session_state:
    st.session_state["max_tokens"] = 256
if "temperature" not in st.session_state:
    st.session_state["temperature"] = 0.7
if "top_p" not in st.session_state:
    st.session_state["top_p"] = 0.9
if "top_k" not in st.session_state:
    st.session_state["top_k"] = 50
if "repeat_penalty" not in st.session_state:
    st.session_state["repeat_penalty"] = 1.1
if "tts_gender" not in st.session_state:  # TTS ì„¤ì •
    st.session_state["tts_gender"] = "female"
if "tts_speed" not in st.session_state:
    st.session_state["tts_speed"] = "normal"
if "recorded_audio" not in st.session_state:  # ë…¹ìŒëœ ì˜¤ë””ì˜¤ ì €ì¥
    st.session_state["recorded_audio"] = None

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    st.session_state.language = st.selectbox("ì–¸ì–´", ["ko", "en", "ja", "zh-CN", "fr", "de"], index=0)
    language_code = st.session_state.language.split("-")[0]

    with st.expander("LLM ì„¤ì •"):
        st.session_state.max_tokens = st.slider("ìµœëŒ€ í† í° ìˆ˜", 1, 2048, 256, 1)
        st.session_state.temperature = st.slider("ì˜¨ë„", 0.1, 4.0, 0.7, 0.1)
        st.session_state.top_p = st.slider("Top P", 0.1, 1.0, 0.9, 0.05)
        st.session_state.top_k = st.slider("Top K", 1, 100, 50, 1)
        st.session_state.repeat_penalty = st.slider("ë°˜ë³µ í˜ë„í‹°", 0.1, 2.0, 1.1, 0.05)

    with st.expander("TTS ì„¤ì •"):
        st.session_state.tts_gender = st.selectbox("ëª©ì†Œë¦¬ ì„±ë³„", ["female", "male"], index=0)
        st.session_state.tts_speed = st.select_slider("ì½ê¸° ì†ë„", ["slow", "normal", "fast"], value="normal")

# --- ë©”ì¸ ì•± ---
st.markdown("<h1 class='title'>í•´ì•¼ í• ê¹Œìš”...? ğŸ¤”</h1>", unsafe_allow_html=True)
st.markdown("<p class='subtitle'>ì§ˆë¬¸ì„ í•˜ê³  í™•ë¥  ê¸°ë°˜ ë¶„ì„ì„ ë°›ìœ¼ì„¸ìš”!</p>", unsafe_allow_html=True)

col1, col2 = st.columns([3, 1])

with col1:
    input_type = st.radio("ì…ë ¥ ìœ í˜•", ["í…ìŠ¤íŠ¸", "ìŒì„±", "ì´ë¯¸ì§€ ì—…ë¡œë“œ", "ì‚¬ì§„ ì´¬ì˜"], horizontal=True)
    question = ""

    if input_type == "í…ìŠ¤íŠ¸":
        question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ì´ ì£¼ì‹ì„ ì‚¬ì•¼ í• ê¹Œìš”?")

    elif input_type == "ìŒì„±":
        # WebRTCë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ì˜¤ë””ì˜¤ ë…¹ìŒ (JavaScript ì‚¬ìš©)
        webrtc_ctx = webrtc_streamer(
            key="speech-recording",
            mode=WebRtcMode.SENDONLY,
            audio_receiver_size=1024,
            media_stream_constraints={"video": False, "audio": True},
        )

        if webrtc_ctx.state.playing:
            st.write("ğŸ¤ ë…¹ìŒ ì¤‘...")
        if webrtc_ctx.audio_receiver:

             # ì˜¤ë””ì˜¤ í”„ë ˆì„ì„ JavaScriptì—ì„œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì  (Python ì½œë°± ìµœì†Œí™”)
            if webrtc_ctx.audio_receiver.frames_queue.qsize() > 0:
                try:
                    audio_frames = webrtc_ctx.audio_receiver.get_frames()

                    if audio_frames:
                        latest_frame = audio_frames[-1]
                        audio_bytes = latest_frame.to_ndarray().tobytes()

                        #  JavaScriptë¡œ ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬
                        st.session_state.recorded_audio = audio_bytes # ì„¸ì…˜ì— ì €ì¥.

                except Exception as e:
                    st.error(f"Error processing audio frames: {e}")

    if input_type == "ì´ë¯¸ì§€ ì—…ë¡œë“œ":
        question = st.text_input("ì§ˆë¬¸ (ì„ íƒ ì‚¬í•­):", placeholder="ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒ ì‚¬í•­).")
        uploaded_image = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"], label_visibility='collapsed')
        camera_image = None
    elif input_type == "ì‚¬ì§„ ì´¬ì˜":
        question = st.text_input("ì§ˆë¬¸ (ì„ íƒ ì‚¬í•­):", placeholder="ì‚¬ì§„ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒ ì‚¬í•­).")
        camera_image = st.camera_input("ì‚¬ì§„ ì´¬ì˜", label_visibility="collapsed")
        uploaded_image = None
    else:
        uploaded_image = None
        camera_image = None

    # JavaScriptë¥¼ ì‚¬ìš©í•˜ì—¬ Speech Recognition API í˜¸ì¶œ (ë¸Œë¼ìš°ì € ë‚´ì—ì„œ)
    if input_type == "ìŒì„±" and st.session_state.recorded_audio:
        js_code = f"""
        async function transcribeAudio() {{
            const audioBlob = new Blob([new Uint8Array(JSON.parse('{json.dumps(list(st.session_state.recorded_audio))}'))], {{ type: 'audio/webm' }}); // ë°”ì´íŠ¸ ë°°ì—´ì„ Blobìœ¼ë¡œ
            const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
            recognition.lang = '{language_code}';
            recognition.continuous = false;
            recognition.interimResults = false;

            recognition.onresult = (event) => {{
                const transcript = event.results[0][0].transcript;
                console.log('Transcript:', transcript);
                // Streamlitì— í…ìŠ¤íŠ¸ ì „ì†¡ (callback ì‚¬ìš© X - Python ë³€ìˆ˜ì— ì§ì ‘ í• ë‹¹)
                document.dispatchEvent(new CustomEvent("set_question", {{detail: transcript}}));

            }};

            recognition.onerror = (event) => {{
                console.error('Recognition error:', event.error);
                 document.dispatchEvent(new CustomEvent("st_error", {{detail: event.error}}));

            }};
            recognition.onend = () => {{
                console.log('Recognition ended.');

            }}

            const stream = await navigator.mediaDevices.getUserMedia({{ audio: true }}); // Stream ë¶ˆí•„ìš”.
            recognition.start();


        }}
        transcribeAudio();
        """
        st.components.v1.html(f"<script>{js_code}</script>", height=0)


    # Streamlit ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ (JavaScriptì—ì„œ Pythonìœ¼ë¡œ ë°ì´í„° ì „ì†¡)
    st.markdown(
        """
        <script>
        document.addEventListener('set_question', function(e) {
            var input = window.parent.document.querySelectorAll('input[type=text]')[0];
            input.value = e.detail;
            input.dispatchEvent(new Event('input')); // Streamlitì— input ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±°

        });
         document.addEventListener('st_error', function(e) {
             // Streamlit python console ì— ë¡œê·¸
            console.error('JS STT:', e.detail);

        });

        </script>
        """,
        unsafe_allow_html=True,
    )


    if st.button("ë¶„ì„", type="primary", use_container_width=True):
        if input_type == "ì´ë¯¸ì§€ ì—…ë¡œë“œ" and not uploaded_image:
            st.warning("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        elif input_type == "ì‚¬ì§„ ì´¬ì˜" and not camera_image:
            st.warning("ì‚¬ì§„ì„ ì´¬ì˜í•´ì£¼ì„¸ìš”.")
        elif not question and input_type in ("í…ìŠ¤íŠ¸", "ìŒì„±"):
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜, ìŒì„±ì„ ë…¹ìŒí•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("ë¶„ì„ ì¤‘..."):
                if input_type in ("í…ìŠ¤íŠ¸", "ìŒì„±") and question:
                    probability, reason, audio = analyze_text(question, st.session_state.language)
                elif input_type == "ì´ë¯¸ì§€ ì—…ë¡œë“œ" and uploaded_image:
                    image_bytes = uploaded_image.getvalue()
                    image_base64 = encode_image(image_bytes)
                    probability, reason, audio = analyze_image(image_base64, question, st.session_state.language)
                elif input_type == "ì‚¬ì§„ ì´¬ì˜" and camera_image:
                    image_bytes = camera_image.getvalue()
                    image_base64 = encode_image(image_bytes)
                    probability, reason, audio = analyze_image(image_base64, question, st.session_state.language)
                else:
                    probability, reason, audio = None, "ì…ë ¥ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None

                with col2:
                    if probability is not None and reason:
                        st.subheader("ë¶„ì„ ê²°ê³¼")
                        with st.container(border=True):
                            if probability >= 50:
                                st.success(f"âœ… ì˜ˆ! ({probability}%)")
                            else:
                                st.error(f"âŒ ì•„ë‹ˆì˜¤! ({probability}%)")
                            st.progress(probability / 100.0)
                            with st.expander("ì´ìœ ", expanded=True):
                                st.markdown(reason)
                            if audio:
                                st.audio(audio, format="audio/mp3")
                    elif reason:
                        st.error(reason)